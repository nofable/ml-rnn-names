{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Model Comparison: RNN vs LSTM\n",
    "\n",
    "This notebook implements a rigorous hyperparameter search and model comparison following proper scientific methodology:\n",
    "\n",
    "1. **Train/Validation/Test split** - Hyperparameters selected by validation loss, test set only touched once\n",
    "2. **Reproducible seeds** - All random operations are seeded\n",
    "3. **Statistical robustness** - Best configs evaluated with multiple seeds, reporting mean +/- std\n",
    "4. **Fair comparison** - Same data split and search budget for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ml_rnn_names.model import CharRNN, CharLSTM\n",
    "from ml_rnn_names.processing import n_letters\n",
    "from ml_rnn_names.training import train, set_seed\n",
    "from ml_rnn_names.evaluation import compute_accuracy, evaluate_model, compute_confusion_matrix, plot_confusion_matrix\n",
    "from ml_rnn_names.utils import torch_device_setup\n",
    "from ml_rnn_names.data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"n_trials\": 10,           # Number of random hyperparameter configurations to try\n",
    "    \"seeds_per_config\": 3,    # Number of seeds for statistical robustness\n",
    "    \"data_seed\": 2024,        # Fixed seed for data split (ensures same split across runs)\n",
    "    \"validation_split\": 0.15, # 70% train, 15% val, 15% test\n",
    "    \"param_ranges\": {\n",
    "        \"hidden_size\": [64, 128, 256],\n",
    "        \"learning_rate\": (0.01, 0.5),\n",
    "        \"n_batch_size\": [32, 64, 128],\n",
    "        \"n_epoch\": [10, 15, 20],\n",
    "    },\n",
    "    \"lstm_extra_params\": {\n",
    "        \"num_layers\": [2, 3, 4],  # LSTM-specific parameter\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with 3-way split (same split for all models)\n",
    "device = torch_device_setup()\n",
    "train_set, val_set, test_set, labels_uniq = load_data(\n",
    "    device,\n",
    "    validation_split=CONFIG[\"validation_split\"],\n",
    "    seed=CONFIG[\"data_seed\"]\n",
    ")\n",
    "\n",
    "print(f\"\\nClasses: {labels_uniq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model factory\n",
    "def create_model(model_type, hidden_size, num_layers=2):\n",
    "    \"\"\"Create a model instance based on type and hyperparameters.\"\"\"\n",
    "    if model_type == \"RNN\":\n",
    "        return CharRNN(\n",
    "            input_size=n_letters,\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=len(labels_uniq)\n",
    "        )\n",
    "    elif model_type == \"LSTM\":\n",
    "        return CharLSTM(\n",
    "            input_size=n_letters,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            output_size=len(labels_uniq)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "search_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model_type, n_trials, param_ranges, train_data, val_data, device, base_seed=0):\n",
    "    \"\"\"\n",
    "    Run random hyperparameter search, selecting by VALIDATION loss.\n",
    "    \n",
    "    Args:\n",
    "        model_type: \"RNN\" or \"LSTM\"\n",
    "        n_trials: number of configurations to try\n",
    "        param_ranges: dict of hyperparameter ranges\n",
    "        train_data: training dataset\n",
    "        val_data: validation dataset\n",
    "        device: torch device\n",
    "        base_seed: base seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        results: list of all trial results\n",
    "        best: best trial (by validation loss)\n",
    "    \"\"\"\n",
    "    set_seed(base_seed, device)\n",
    "    results = []\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Sample hyperparameters\n",
    "        hidden_size = int(np.random.choice(param_ranges[\"hidden_size\"]))\n",
    "        learning_rate = np.random.uniform(*param_ranges[\"learning_rate\"])\n",
    "        n_batch_size = int(np.random.choice(param_ranges[\"n_batch_size\"]))\n",
    "        n_epoch = int(np.random.choice(param_ranges[\"n_epoch\"]))\n",
    "        \n",
    "        # LSTM-specific: sample num_layers\n",
    "        num_layers = 2\n",
    "        if model_type == \"LSTM\" and \"num_layers\" in CONFIG.get(\"lstm_extra_params\", {}):\n",
    "            num_layers = int(np.random.choice(CONFIG[\"lstm_extra_params\"][\"num_layers\"]))\n",
    "        \n",
    "        print(f\"Trial {trial + 1}/{n_trials}: hidden={hidden_size}, lr={learning_rate:.4f}, \"\n",
    "              f\"batch={n_batch_size}, epochs={n_epoch}\" + \n",
    "              (f\", layers={num_layers}\" if model_type == \"LSTM\" else \"\"))\n",
    "        \n",
    "        model = create_model(model_type, hidden_size, num_layers)\n",
    "        \n",
    "        result = train(\n",
    "            model=model,\n",
    "            training_data=train_data,\n",
    "            validation_data=val_data,\n",
    "            n_epoch=n_epoch,\n",
    "            n_batch_size=n_batch_size,\n",
    "            learning_rate=learning_rate,\n",
    "            device=device,\n",
    "            seed=base_seed + trial,\n",
    "            report_every=100,  # Reduce verbosity\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"model_type\": model_type,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"n_batch_size\": n_batch_size,\n",
    "            \"n_epoch\": n_epoch,\n",
    "            \"final_train_loss\": result[\"train_losses\"][-1],\n",
    "            \"final_val_loss\": result[\"val_losses\"][-1],\n",
    "            \"train_losses\": result[\"train_losses\"],\n",
    "            \"val_losses\": result[\"val_losses\"],\n",
    "        })\n",
    "        \n",
    "        print(f\"  -> train_loss={result['train_losses'][-1]:.4f}, val_loss={result['val_losses'][-1]:.4f}\\n\")\n",
    "    \n",
    "    # Select by VALIDATION loss (not training loss!)\n",
    "    best = min(results, key=lambda x: x[\"final_val_loss\"])\n",
    "    return results, best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "search_header",
   "metadata": {},
   "source": [
    "## Hyperparameter Search\n",
    "\n",
    "Run random search for each model type, selecting the best configuration by **validation loss**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "best_configs = {}\n",
    "\n",
    "for model_type in [\"RNN\", \"LSTM\"]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Searching hyperparameters for {model_type}\")\n",
    "    print('='*60 + \"\\n\")\n",
    "    \n",
    "    results, best = random_search(\n",
    "        model_type=model_type,\n",
    "        n_trials=CONFIG[\"n_trials\"],\n",
    "        param_ranges=CONFIG[\"param_ranges\"],\n",
    "        train_data=train_set,\n",
    "        val_data=val_set,\n",
    "        device=device,\n",
    "        base_seed=42 if model_type == \"RNN\" else 142,  # Different base seeds per model\n",
    "    )\n",
    "    \n",
    "    all_results[model_type] = results\n",
    "    best_configs[model_type] = best\n",
    "    \n",
    "    print(f\"\\nBest {model_type} config (selected by validation loss):\")\n",
    "    for k, v in best.items():\n",
    "        if k not in [\"train_losses\", \"val_losses\"]:\n",
    "            print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robustness_header",
   "metadata": {},
   "source": [
    "## Statistical Robustness\n",
    "\n",
    "Retrain the best configuration for each model with multiple random seeds to assess variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi_seed_eval",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_multiple_seeds(config, train_data, val_data, test_data, seeds, device):\n",
    "    \"\"\"\n",
    "    Train the same config with multiple seeds and report statistics.\n",
    "    \n",
    "    Returns dict with mean and std of test metrics.\n",
    "    \"\"\"\n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "    trained_models = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"  Training with seed {seed}...\")\n",
    "        model = create_model(\n",
    "            config[\"model_type\"],\n",
    "            config[\"hidden_size\"],\n",
    "            config.get(\"num_layers\", 2)\n",
    "        )\n",
    "        \n",
    "        train(\n",
    "            model=model,\n",
    "            training_data=train_data,\n",
    "            validation_data=val_data,\n",
    "            n_epoch=config[\"n_epoch\"],\n",
    "            n_batch_size=config[\"n_batch_size\"],\n",
    "            learning_rate=config[\"learning_rate\"],\n",
    "            device=device,\n",
    "            seed=seed,\n",
    "            report_every=100,\n",
    "        )\n",
    "        \n",
    "        # Evaluate on TEST set (only now do we touch test data)\n",
    "        metrics = evaluate_model(model, train_data, val_data, test_data)\n",
    "        test_accuracies.append(metrics[\"test_accuracy\"])\n",
    "        test_losses.append(metrics[\"test_loss\"])\n",
    "        trained_models.append(model)\n",
    "        \n",
    "        print(f\"    test_accuracy={metrics['test_accuracy']:.4f}, test_loss={metrics['test_loss']:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"test_accuracy_mean\": np.mean(test_accuracies),\n",
    "        \"test_accuracy_std\": np.std(test_accuracies),\n",
    "        \"test_loss_mean\": np.mean(test_losses),\n",
    "        \"test_loss_std\": np.std(test_losses),\n",
    "        \"models\": trained_models,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_robustness",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [42, 123, 456]\n",
    "final_results = {}\n",
    "\n",
    "for model_type, config in best_configs.items():\n",
    "    print(f\"\\nEvaluating {model_type} with {len(seeds)} seeds...\")\n",
    "    stats = evaluate_with_multiple_seeds(\n",
    "        config, train_set, val_set, test_set, seeds, device\n",
    "    )\n",
    "    final_results[model_type] = stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results_header",
   "metadata": {},
   "source": [
    "## Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results_table",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS (Test Set)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Model':<10} {'Accuracy':<25} {'Loss':<25}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for model_type, stats in final_results.items():\n",
    "    acc = f\"{stats['test_accuracy_mean']*100:.1f}% +/- {stats['test_accuracy_std']*100:.1f}%\"\n",
    "    loss = f\"{stats['test_loss_mean']:.4f} +/- {stats['test_loss_std']:.4f}\"\n",
    "    print(f\"{model_type:<10} {acc:<25} {loss:<25}\")\n",
    "\n",
    "print(\"\\nBest hyperparameters used:\")\n",
    "for model_type, config in best_configs.items():\n",
    "    print(f\"\\n{model_type}:\")\n",
    "    for k in [\"hidden_size\", \"num_layers\", \"learning_rate\", \"n_batch_size\", \"n_epoch\"]:\n",
    "        if k in config:\n",
    "            print(f\"  {k}: {config[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_header",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation loss across all trials for each model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, model_type in enumerate([\"RNN\", \"LSTM\"]):\n",
    "    ax = axes[idx]\n",
    "    for result in all_results[model_type]:\n",
    "        ax.plot(result[\"val_losses\"], alpha=0.5, \n",
    "                label=f\"h={result['hidden_size']}, lr={result['learning_rate']:.2f}\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Validation Loss\")\n",
    "    ax.set_title(f\"{model_type} - Hyperparameter Search\")\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion_matrices",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for best models (using first seed's model)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "for idx, model_type in enumerate([\"RNN\", \"LSTM\"]):\n",
    "    model = final_results[model_type][\"models\"][0]  # First seed's model\n",
    "    confusion = compute_confusion_matrix(model, test_set, len(labels_uniq))\n",
    "    \n",
    "    # Normalize\n",
    "    for i in range(len(labels_uniq)):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    cax = ax.matshow(confusion.cpu().numpy())\n",
    "    fig.colorbar(cax, ax=ax)\n",
    "    ax.set_xticks(np.arange(len(labels_uniq)))\n",
    "    ax.set_yticks(np.arange(len(labels_uniq)))\n",
    "    ax.set_xticklabels(labels_uniq, rotation=90)\n",
    "    ax.set_yticklabels(labels_uniq)\n",
    "    ax.set_title(f\"{model_type} Confusion Matrix (Test Set)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
