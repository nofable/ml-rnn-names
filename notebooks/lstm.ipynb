{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "884bf2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from ml_rnn_names.model import CharLSTM\n",
    "from ml_rnn_names.processing import n_letters\n",
    "from ml_rnn_names.training import train\n",
    "from ml_rnn_names.utils import torch_device_setup\n",
    "from ml_rnn_names.data import load_data, collate_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9f653",
   "metadata": {},
   "outputs": [],
   "source": "# Perform Random Search to find suitable hyperparameters\ndevice = torch_device_setup()\ntrain_set, _, test_set, labels_uniq = load_data(device)\n\nn_trials = 5  # Number of random combinations to try\n\nparam_ranges = {\n    \"hidden_size\": [64, 128, 256, 512],  # discrete choices\n    \"learning_rate\": (0.001, 0.5),  # (min, max) for uniform sampling\n    \"n_batch_size\": [32, 64, 128],  # discrete choices\n    \"num_layers\": (2, 5),  # number of lstm layers\n    \"n_epoch\": (5, 20),  # will be converted to int\n}  # I removed \"learning_rate\" to try multiple below\n\nresults = []\nplt.figure(figsize=(12, 6))\n\nfor trial in range(n_trials):\n    # Randomly sample hyperparameters\n    hidden_size = int(np.random.choice(param_ranges[\"hidden_size\"]))\n    num_layers = int(np.random.uniform(*param_ranges[\"num_layers\"]))\n    learning_rate = np.random.uniform(*param_ranges[\"learning_rate\"])\n    n_batch_size = int(np.random.choice(param_ranges[\"n_batch_size\"]))\n    n_epoch = int(np.random.uniform(*param_ranges[\"n_epoch\"]))\n\n    model_parameters = {\n        \"input_size\": n_letters,\n        \"output_size\": len(labels_uniq),\n        \"num_layers\": num_layers,\n        \"hidden_size\": hidden_size,\n    }\n    hyper_parameters = {\n        \"n_epoch\": n_epoch,\n        \"n_batch_size\": n_batch_size,\n        \"learning_rate\": learning_rate,\n    }\n\n    print(\n        f\"Trial {trial + 1}/{n_trials}: LR={learning_rate:.4f}, \"\n        f\"Hidden={hidden_size}, Batch={n_batch_size}, Epochs={n_epoch}\"\n    )\n\n    lstm = CharLSTM(**model_parameters)\n    result = train(\n        model=lstm, training_data=train_set, **hyper_parameters, device=device\n    )\n    all_losses = result[\"train_losses\"]\n\n    # Store results\n    results.append(\n        {\n            \"learning_rate\": learning_rate,\n            \"hidden_size\": hidden_size,\n            \"n_batch_size\": n_batch_size,\n            \"n_epoch\": n_epoch,\n            \"final_loss\": all_losses[-1],\n            \"all_losses\": all_losses,\n        }\n    )\n\n    plt.plot(all_losses, alpha=0.5, label=f\"LR={learning_rate:.3f}, H={hidden_size}\")\n\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss for Random Search\")\nplt.tight_layout()\nplt.show()\n\n# Find best hyperparameters\nbest_trial = min(results, key=lambda x: x[\"final_loss\"])\nprint(\"\\nBest hyperparameters:\")\nfor key, value in best_trial.items():\n    if key != \"all_losses\":\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe573916",
   "metadata": {},
   "outputs": [],
   "source": "device = torch_device_setup()\ntrain_set, _, test_set, labels_uniq = load_data(device)\n\nmodel_parameters = {\n    \"input_size\": n_letters,\n    \"output_size\": len(labels_uniq),\n    \"hidden_size\": 128,\n    \"num_layers\": 2,\n}\nhyper_parameters = {\n    \"n_epoch\": 16,\n    \"n_batch_size\": 32,\n    \"learning_rate\": 0.215,\n}\n\n\nlstm = CharLSTM(**model_parameters)\nresult = train(\n    model=lstm, training_data=train_set, **hyper_parameters, device=device\n)\nall_losses = result[\"train_losses\"]\n\nplt.plot(all_losses, alpha=0.5, label=f\"LR={hyper_parameters['learning_rate']:.3f}, H={model_parameters['hidden_size']}\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss\")\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c86ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def evaluate(model, testing_data, classes):\n",
    "    confusion = torch.zeros(len(classes), len(classes))\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        testing_data,\n",
    "        batch_size=64,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_names,\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for labels, padded_texts, lengths in dataloader:\n",
    "            outputs = model(padded_texts, lengths)\n",
    "            _, predictions = outputs.topk(1)\n",
    "            predictions = predictions.squeeze(1)\n",
    "\n",
    "            for label, pred in zip(labels, predictions):\n",
    "                confusion[label][pred] += 1\n",
    "\n",
    "    # Normalize by dividing every row by its sum\n",
    "    for i in range(len(classes)):\n",
    "        denom = confusion[i].sum()\n",
    "        if denom > 0:\n",
    "            confusion[i] = confusion[i] / denom\n",
    "\n",
    "    # Set up plot\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(confusion.cpu().numpy())\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticks(np.arange(len(classes)), labels=classes, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(classes)), labels=classes)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate(lstm, test_set, classes=labels_uniq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-rnn-names",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}